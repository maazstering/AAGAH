{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_AZJ8xL_y9g","executionInfo":{"status":"ok","timestamp":1710811204823,"user_tz":420,"elapsed":5713,"user":{"displayName":"Danial Zahid","userId":"14115216571533553163"}},"outputId":"cbd9ba21-bb40-4f5e-d9fc-e8c6c281a608"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"]}],"source":["pip install unidecode"]},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","import requests\n","from urllib.parse import urljoin\n","import re\n","import time\n","import pandas as pd\n","import os\n","from unidecode import unidecode\n","myheader = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\",  \"Accept\":\"text/html,application/xhtml+xml,application/xml; q=0.9,image/webp,image/apng,*/*;q=0.8\"}"],"metadata":{"id":"oV10Y6bUCk1Y","executionInfo":{"status":"ok","timestamp":1710811204823,"user_tz":420,"elapsed":15,"user":{"displayName":"Danial Zahid","userId":"14115216571533553163"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def extractLinks(root_url, pattern=None):\n","\n","  try:\n","    html = requests.get(root_url, headers=myheader)\n","    html.raise_for_status()\n","\n","    # Create a BS object to parse the HTML content\n","    bsObj = BeautifulSoup(html.content, \"lxml\")\n","\n","    links = set()\n","\n","    # Iterate through all 'a' tags with 'href' attribute\n","    for link in bsObj.find_all(\"a\", href=True):\n","      href = link[\"href\"]\n","\n","      # Check if the link matches the specified pattern using regular expressions('http://' or 'https://'))\n","      if re.match(pattern, href) or pattern is None:\n","        if href.startswith((\"http://\", \"https://\", \"www.\")):\n","          links.add(href)\n","        else:\n","          links.add(urljoin(root_url, href))\n","\n","    return links\n","\n","  #Exception handling\n","  except requests.RequestException as e:\n","    print(f\"Error during HTTP request for {root_url}: {e}\")\n","    return set()\n","\n","  except Exception as e:\n","    print(f\"An unexpected error occurred for {root_url}: {e}\")\n","    return set()\n"],"metadata":{"id":"AGG0tesfCnhv","executionInfo":{"status":"ok","timestamp":1710811204823,"user_tz":420,"elapsed":11,"user":{"displayName":"Danial Zahid","userId":"14115216571533553163"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def scrapeAndSave(links, category, element, classOrId, find_all_flag=False):\n","    articleSet = set()\n","\n","    for link in links:\n","        try:\n","            article = requests.get(link, headers=myheader)\n","            article.raise_for_status()\n","\n","            soup = BeautifulSoup(article.content, \"html.parser\")\n","\n","            if classOrId == \"class\":\n","                if find_all_flag:\n","                    bodyElements = soup.find_all(class_=element)\n","                else:\n","                    bodyElements = [soup.find(class_=element)]\n","            else:\n","                if find_all_flag:\n","                    bodyElements = soup.find_all(id=element)\n","                else:\n","                    bodyElements = [soup.find(id=element)]\n","\n","            for bodyElement in bodyElements:\n","                if bodyElement:\n","                    # If the element exists, extract its text\n","                    bodyText = bodyElement.text\n","                    cleanedText = unidecode(bodyText).strip()\n","                    articleSet.add((cleanedText, category, link))\n","\n","                else:\n","                    print(f\"Could not find body element for link: {link}\")\n","                    articleSet.add((\"Not available\", \"N/A\", link))\n","\n","            time.sleep(1)\n","\n","        except requests.RequestException as e:\n","            print(f\"Error during HTTP request for link {link}: {e}\")\n","            continue\n","\n","        except Exception as e:\n","            print(f\"An unexpected error occurred for link {link}: {e}\")\n","            continue\n","\n","    return articleSet"],"metadata":{"id":"71R5xF-0CvhI","executionInfo":{"status":"ok","timestamp":1710811204824,"user_tz":420,"elapsed":10,"user":{"displayName":"Danial Zahid","userId":"14115216571533553163"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def convertToCsv(data, filename):\n","    file_exists = os.path.exists(filename)\n","\n","    # If the file exists, read it into a DataFrame\n","    if file_exists:\n","        existing_df = pd.read_csv(filename)\n","    else:\n","        existing_df = pd.DataFrame(columns=[\"Description\"])\n","\n","    # New DataFrame with the new data\n","    new_df = pd.DataFrame({\"Description\": [info[0] for info in data],\n","                           \"Department\": [info[1] for info in data],\n","                           \"Link\": [info[2] for info in data]})\n","\n","    is_duplicate = new_df.isin(existing_df.to_dict(orient='list')).all(axis=1).any()\n","\n","    # Update the new data to the existing DataFrame and write to CSV\n","    if not is_duplicate:\n","        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n","        combined_df.to_csv(filename, index=False, mode='w' if file_exists else 'a', header=not file_exists)\n","    else:\n","        print(\"Data is already present in the CSV file. Not adding duplicates.\")"],"metadata":{"id":"Z2j-MVoeCxvH","executionInfo":{"status":"ok","timestamp":1710811204824,"user_tz":420,"elapsed":9,"user":{"displayName":"Danial Zahid","userId":"14115216571533553163"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","def extract_text_from_website(url):\n","    try:\n","        # Fetch\n","        response = requests.get(url)\n","        response.raise_for_status()\n","\n","        # Parse\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # Extract text from all the paragraphs excluding those containing \"Read more\" or \"Continue reading\"\n","        paragraphs = soup.find_all('p')\n","        extracted_text = '\\n'.join([p.get_text() for p in paragraphs if \"Read More...\" not in p.get_text() and \"Continue reading\" not in p.get_text()])\n","\n","        return extracted_text\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","url = \"https://www.bbc.com/news/topics/c008ql15vpyt\"\n","extracted_text = extract_text_from_website(url)\n","if extracted_text:\n","    print(extracted_text)\n","else:\n","    print(\"Failed to extract text from the website.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qcQ9rXwbqgMA","executionInfo":{"status":"ok","timestamp":1710811205384,"user_tz":420,"elapsed":569,"user":{"displayName":"Danial Zahid","userId":"14115216571533553163"}},"outputId":"e11e4a48-c949-4f13-f59d-1f5903db636f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["The Taliban accuses Pakistan of killing eight women and children after an attack killed Pakistani troops.\n","Provides an overview of Pakistan, including key dates and facts about this south Asian country.\n","Judges in Pakistan say he shared blasphemous pictures and videos to outrage the feelings of Muslims.\n","Seven years after Ishrat Baig donated £40k to a charity to build an orphanage, it is still incomplete.\n","The icy conditions are unexpected as the region is typically humid in March.\n","Mr Sharif got parliament's backing after a deal shut out supporters of his jailed predecessor Imran Khan.\n","Pakistan's army has long wielded an unchallenged amount of power over politics - until now.\n","Hundreds mistakenly accused the woman of wearing a dress adorned with Quran verses.\n","It comes after a contentious election where the largest party led by former-PM Imran Khan alleged vote-rigging.\n","Copyright 2024 BBC. All rights reserved.  The BBC is not responsible for the content of external sites. Read about our approach to external linking.\n"," \n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","def extract_text_from_website(url):\n","    try:\n","        # Fetch\n","        response = requests.get(url)\n","        response.raise_for_status()\n","\n","        # Parse\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # Extract text from all the paragraphs excluding those containing \"Read more\" or \"Continue reading\"\n","        paragraphs = soup.find_all('p')\n","        extracted_text = '\\n'.join([p.get_text() for p in paragraphs if \"Read More...\" not in p.get_text() and \"Continue reading\" not in p.get_text()])\n","\n","        return extracted_text\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","def save_to_csv(data, filename='extracted_text.csv'):\n","    try:\n","        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n","            writer = csv.writer(csvfile)\n","            writer.writerow(['Extracted Text'])\n","            for row in data.split('\\n'):\n","                writer.writerow([row])\n","        print(f\"Data saved to {filename} successfully.\")\n","    except Exception as e:\n","        print(f\"An error occurred while saving to CSV: {e}\")\n","\n","url = \"https://www.bbc.com/news/topics/c008ql15vpyt\"\n","extracted_text = extract_text_from_website(url)\n","if extracted_text:\n","    save_to_csv(extracted_text)\n","else:\n","    print(\"Failed to extract text from the website.\")"],"metadata":{"id":"w7egomCUq18q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710811205385,"user_tz":420,"elapsed":12,"user":{"displayName":"Danial Zahid","userId":"14115216571533553163"}},"outputId":"291c9b7a-a4fb-4c3e-ec47-1d1beb705d81"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Data saved to extracted_text.csv successfully.\n"]}]}]}